# MMMU-Pro Inference and Evaluation with Qwen2.5-VL

This script provides tools to run inference using a Qwen2.5-VL model on the MMMU Pro dataset and evaluate the generated results. Evaluation can be performed using different judge models via DashScope or a custom MIT API endpoint.

## Overview

The script operates in two main modes:

1.  **`infer`**: Runs inference on specified subsets (`original`, `vision`) of the MMMU-Pro dataset using a provided Qwen2.5-VL model checkpoint. It generates predictions for each question in the dataset subset.
2.  **`eval`**: Takes the inference results (in JSONL format) and evaluates them against the ground truth answers. It supports different evaluation methods, including exact matching and using LLM-based judges like GPT-4 via API calls.

## Features

*   Handles both `original` (multiple-choice questions with images) and `vision` (vision-centric tasks) subsets of MMMU-Pro.
*   Utilizes the `Qwen2VLChat` model implementation for inference.
*   Supports custom system prompts.
*   Configurable image processing parameters (`min_pixels`, `max_pixels`).
*   Configurable generation parameters (`temperature`, `top_p`, `top_k`).
*   Evaluates predictions using either `exact_matching` or external judge models (e.g., `gpt-4-0125-preview`, `qwen-plus`).
*   Supports different API backends for judge models (Dashscope, MIT Spider/OpenAI).
*   Outputs detailed inference results in JSONL format and aggregated accuracy scores in CSV format.

## Setup

1.  **Clone the Repository:**
    ```bash
    # git clone <repository_url>
    # cd <repository_directory>
    ```

2.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Prepare Model:**
    *   Download the Qwen2.5-VL model checkpoint from HuggingFace.

4.  **Set Environment Variables (for Evaluation):**
    *   **DashScope:**
        ```bash
        export CHATGPT_DASHSCOPE_API_KEY="your-dashscope-api-key"
        export DASHSCOPE_API_BASE="your-dashscope-api-base"
        ```
    *   **MIT API:**
        ```bash
        export MIT_SPIDER_TOKEN="your-mit-spider-token"
        export MIT_SPIDER_URL="your-mit-spider-url"

## Usage

### 1. Inference

Run inference on the desired subset.

**Command for the `original` subset:**

```bash
python run_mmmu_pro.py infer \
    --model-path /path/to/your/qwen2.5-vl/model \
    --data-dir /path/to/MMMU_Pro \
    --subset original \
    --output-path ./mmmupro_original_results.jsonl \
    # Optional: Add --temperature, --top-p, --top-k, --sys-prompt if needed
```

**Command for the `vision` subset:**

```bash
python run_mmmu_pro.py infer \
    --model-path /path/to/your/qwen2.5-vl/model \
    --data-dir /path/to/MMMU_Pro \
    --subset vision \
    --output-path ./mmmupro_vision_results.jsonl \
    # Optional: Add --temperature, --top-p, --top-k, --sys-prompt if needed
```

*   Replace `/path/to/your/qwen2.5-vl/model` with the actual path to your downloaded Qwen2.5-VL model.
*   Replace `/path/to/MMMU_Pro` with the path to your MMMU-Pro dataset directory if it's not named `MMMU_Pro` in the current directory.
*   Specify your desired output file path (e.g., `./mmmupro_vision_results.jsonl`). The script will append results if the file already exists.

### 2. Evaluation

Evaluate the results generated during the inference step. **Remember to set the API keys first if using an LLM judge.**

**Command for Evaluation:**

```bash
# Ensure API keys are set via export (see Setup step 4) if using LLM judge

python run_mmmu_pro.py eval \
    --input-file ./mmmupro_original_results.jsonl \
    --output-file ./mmmupro_original_scores.csv \
    --eval-model gpt-4-0125-preview \
    --api-type mit \
    --nproc 4
```

*   Replace `./mmmupro_original_results.jsonl` with the actual path to the JSONL file generated by the `infer` step.
*   Specify the desired output path for the accuracy scores CSV file (e.g., `./mmmupro_original_scores.csv`).
*   Choose the evaluation model/method using `--eval-model`. Options include `exact_matching`, `chatgpt-0125`, `gpt-4-0125`, `qwen-plus`, `gpt-4-turbo`, `gpt-4o`, `gpt-4`, `gpt-4-0125-preview`.
*   Specify the API type (`--api-type`) corresponding to your setup (`dash` for Dashscope, `mit` for MIT Spider/OpenAI endpoint). This might depend on the `build_judge` implementation.
*   Adjust the number of processes (`--nproc`) based on your system resources and API rate limits.

## Command-Line Arguments

### General Arguments

*   `mode`: The mode to run the script in. Choices: `infer`, `eval`. (Required)

### Inference Arguments (`--mode infer`)

*   `--model-path` (str, required): Path to the Qwen2-VL model directory.
*   `--data-dir` (str, default: "MMMU_Pro"): Directory containing the MMMU-Pro dataset.
*   `--subset` (str, default: "original", choices: ["original", "vision"]): MMMU-Pro subset to process.
*   `--output-path` (str, required): Output JSONL file path for inference results. Results are appended if the file exists.
*   `--min-pixels` (int, optional): Minimum pixels for image processing.
*   `--max-pixels` (int, optional): Maximum pixels for image processing.
*   `--sys-prompt` (str, optional): System prompt string or path to a file containing the system prompt.
*   `--temperature` (float, default: 0.01): Sampling temperature for generation.
*   `--top-p` (float, default: 0.001): Top-p (nucleus) sampling probability.
*   `--top-k` (int, default: 1): Top-k sampling parameter.

### Evaluation Arguments (`--mode eval`)

*   `--input-file` (str, required): Path to the input JSONL file containing inference results (output from `infer` mode).
*   `--output-file` (str, required): Path to the output CSV file where accuracy scores will be saved.
*   `--eval-model` (str, default: "exact_matching", choices: ['chatgpt-0125', 'exact_matching', 'gpt-4-0125', 'qwen-plus', 'gpt-4-turbo', 'gpt-4o', 'gpt-4', 'gpt-4-0125-preview']): Model or method used for judging answers.
*   `--api-type` (str, default: "dash", choices: ["dash", "mit"]): API type for the judge model (if applicable).
*   `--nproc` (int, default: 4): Number of parallel processes to use for scoring.

## Output Files

*   **Inference Output (`.jsonl`)**: Specified by `--output-path`. Contains one JSON object per line, each representing an inference result for a sample in the dataset. Includes question details, ground truth, model prediction, and potentially error messages.
*   **Evaluation Output (`.csv`)**: Specified by `--output-file`. Contains the accuracy scores, potentially broken down by category, along with overall accuracy.

